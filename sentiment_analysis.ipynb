{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "82595d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import shuffle\n",
    "from scraper.reddit_scraper import Scraper\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "140be360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helpful variables\n",
    "cred_file = \"creds.json\"\n",
    "submission_id = \"qo95nt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2c81c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load credentials\n",
    "with open(cred_file) as f:\n",
    "    creds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "04efd492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Scraper object and connect to Reddit\n",
    "scraper = Scraper(creds, submission_id)\n",
    "scraper.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "64997288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the top level comments for the provided submission\n",
    "comments = scraper.get_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "66f84ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis setup\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment(comment: str):\n",
    "    return sia.polarity_scores(comment)['compound']\n",
    "\n",
    "def is_positive(comment: str):\n",
    "    return sentiment(comment) > 0;\n",
    "\n",
    "class Word_Replacement(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "        \n",
    "    def replace_rep(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace_rep(repl_word)\n",
    "        else:\n",
    "            return repl_word\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "33de9db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleanup\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\") + [\"like\", \"get\"]\n",
    "rep_word = Word_Replacement()\n",
    "\n",
    "words = [j.lower() for i in [w.split() for w in comments] for j in i]\n",
    "filtered_words = [rep_word.replace_rep(w) for w in words if w.isalpha() and w not in stopwords]\n",
    "\n",
    "#various algorithms to get the base of each\n",
    "porter = nltk.stem.PorterStemmer()\n",
    "porter_words = [porter.stem(w) for w in filtered_words]\n",
    "\n",
    "lancaster = nltk.stem.LancasterStemmer()\n",
    "lancaster_words = [lancaster.stem(w) for w in filtered_words]\n",
    "\n",
    "snowball = nltk.stem.SnowballStemmer('english')\n",
    "snowball_words = [snowball.stem(w) for w in filtered_words]\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemma_words = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "\n",
    "cleaned = [p for p in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "36ce49fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total: 2162 negative: 1350 positive: 812\n"
     ]
    }
   ],
   "source": [
    "#Analysis Time!\n",
    "\n",
    "#number of positive vs negative comments\n",
    "pos = 0\n",
    "neg = 0\n",
    "\n",
    "for c in cleaned:\n",
    "    if is_positive(c): pos += 1 \n",
    "    else: neg += 1\n",
    "\n",
    "print(\"\\ntotal: %d negative: %d positive: %d\"%(len(cleaned), neg, pos)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5ba8ad54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fuck fucking    game    good     lol \n",
      "    123      99      91      87      74 \n",
      "None\n",
      "fuck play game look good \n",
      " 228  111  109   93   87 \n",
      "None\n",
      "fuck play  gam look good \n",
      " 230  132  109   93   87 \n",
      "None\n",
      "fuck play game look good \n",
      " 228  111  109   93   87 \n",
      "None\n",
      "   fuck    game fucking    good    goal \n",
      "    124     109      99      87      85 \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Words used most frequently\n",
    "print(nltk.FreqDist(filtered_words).tabulate(5))\n",
    "print(nltk.FreqDist(porter_words).tabulate(5))\n",
    "print(nltk.FreqDist(lancaster_words).tabulate(5))\n",
    "print(nltk.FreqDist(snowball_words).tabulate(5))\n",
    "print(nltk.FreqDist(lemma_words).tabulate(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1b524c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ('look', 'like')   ('holy', 'fuck')   ('feel', 'like') ('fuck', 'boston')    ('love', 'see') \n",
      "                18                 15                 13                 13                 12 \n",
      "('fucking', 'good', 'fucking')    ('good', 'fucking', 'good')        ('call', 'ice', 'goal')      ('game', 'get', 'scored') \n",
      "                             3                              3                              3                              2 \n",
      "('fucking', 'good', 'fucking', 'good')   ('mcavoy', 'wear', 'little', 'halo')     ('wear', 'little', 'halo', 'head') \n",
      "                                     3                                      2                                      2 \n"
     ]
    }
   ],
   "source": [
    "#Word combinations\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(lemma_words)\n",
    "bigram_finder.ngram_fd.tabulate(5)\n",
    "\n",
    "trigram_finder = nltk.collocations.TrigramCollocationFinder.from_words(lemma_words)\n",
    "trigram_finder.ngram_fd.tabulate(4)\n",
    "\n",
    "quadgram_finder = nltk.collocations.QuadgramCollocationFinder.from_words(lemma_words)\n",
    "quadgram_finder.ngram_fd.tabulate(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f3482b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 total occurrences, 13 positive 38 negative\n"
     ]
    }
   ],
   "source": [
    "#context\n",
    "refs_concordance = nltk.Text(lancaster_words).concordance_list(lancaster.stem(\"refs\"), lines=None)\n",
    "print(len(refs_concordance), \"total occurrences,\", end=\" \")\n",
    "pos = 0\n",
    "neg = 0\n",
    "for entry in refs_concordance:\n",
    "    if is_positive(entry.line): pos += 1\n",
    "    else: neg += 1\n",
    "\n",
    "print(pos, \"positive\", neg, \"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb21e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26579191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
