{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "507348d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T10:47:40.238368Z",
     "start_time": "2021-11-10T10:47:40.236281Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from random import shuffle\n",
    "from scraper.reddit_scraper import Scraper\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "28261bc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T10:55:04.941881Z",
     "start_time": "2021-11-10T10:55:04.611200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2139, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hjmmzs1</th>\n",
       "      <td>co7nw</td>\n",
       "      <td>I can appreciate and recognize talent. Hate wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>1636252094.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hjmc5sj</th>\n",
       "      <td>3w83nxpr</td>\n",
       "      <td>Well that last goal, kind of makes this convo ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1636246832.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hjmccym</th>\n",
       "      <td>1523af</td>\n",
       "      <td>Fair enough. Save yourself the absolute bewild...</td>\n",
       "      <td>1</td>\n",
       "      <td>1636246928.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hjmbq65</th>\n",
       "      <td>zn07m</td>\n",
       "      <td>I usually order by what looks good on other pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>1636246627.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hjmcmih</th>\n",
       "      <td>zn07m</td>\n",
       "      <td>We had the convo, which the hockey gods decide...</td>\n",
       "      <td>1</td>\n",
       "      <td>1636247055.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               body score  \\\n",
       "hjmmzs1     co7nw  I can appreciate and recognize talent. Hate wh...     1   \n",
       "hjmc5sj  3w83nxpr  Well that last goal, kind of makes this convo ...     1   \n",
       "hjmccym    1523af  Fair enough. Save yourself the absolute bewild...     1   \n",
       "hjmbq65     zn07m  I usually order by what looks good on other pe...     1   \n",
       "hjmcmih     zn07m  We had the convo, which the hockey gods decide...     1   \n",
       "\n",
       "                 time  \n",
       "hjmmzs1  1636252094.0  \n",
       "hjmc5sj  1636246832.0  \n",
       "hjmccym  1636246928.0  \n",
       "hjmbq65  1636246627.0  \n",
       "hjmcmih  1636247055.0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from file\n",
    "filename = \"data/qo95nt_20211110093614503123.json\"\n",
    "\n",
    "df = pd.read_json(filename).T\n",
    "print(\"shape:\", df.shape)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "897795c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T10:56:44.281142Z",
     "start_time": "2021-11-10T10:56:44.271816Z"
    }
   },
   "outputs": [],
   "source": [
    "#Analysis setup\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment(comment: str):\n",
    "    return sia.polarity_scores(comment)['compound']\n",
    "\n",
    "def is_positive(comment: str):\n",
    "    return sentiment(comment) > 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c033dff2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T11:38:18.947252Z",
     "start_time": "2021-11-10T11:38:18.825701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "      <th>words</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hjmbq2n</th>\n",
       "      <td>2wtn1y1m</td>\n",
       "      <td>Auston Matthews scoring a powerplay goal on a ...</td>\n",
       "      <td>45</td>\n",
       "      <td>1636246625.0</td>\n",
       "      <td>[Auston, Matthews, scoring, a, powerplay, goal...</td>\n",
       "      <td>[auston, matthew, score, powerplay, goal, marc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hjmd23t</th>\n",
       "      <td>41bt33yx</td>\n",
       "      <td>Yo. Jack Campbell keeps the record. Canes lost...</td>\n",
       "      <td>43</td>\n",
       "      <td>1636247261.0</td>\n",
       "      <td>[Yo., Jack, Campbell, keeps, the, record., Can...</td>\n",
       "      <td>[jack, campbel, keep, cane, lost]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hjmi3w6</th>\n",
       "      <td>5i88ndky</td>\n",
       "      <td>Beat the bruins and as soon as it cuts to the ...</td>\n",
       "      <td>40</td>\n",
       "      <td>1636249695.0</td>\n",
       "      <td>[Beat, the, bruins, and, as, soon, as, it, cut...</td>\n",
       "      <td>[beat, bruin, soon, cut, hab, game, score, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hjmi4ch</th>\n",
       "      <td>5aceky7e</td>\n",
       "      <td>Hell week results: Leafs 3-0.</td>\n",
       "      <td>36</td>\n",
       "      <td>1636249701.0</td>\n",
       "      <td>[Hell, week, results:, Leafs, 3-0.]</td>\n",
       "      <td>[hell, week, leaf]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hjliwp1</th>\n",
       "      <td>l8jzj</td>\n",
       "      <td>First game I’m going to in like 8 years. LFG GLG</td>\n",
       "      <td>34</td>\n",
       "      <td>1636233429.0</td>\n",
       "      <td>[First, game, I’m, going, to, in, like, 8, yea...</td>\n",
       "      <td>[first, game, go, lfg, glg]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               body score  \\\n",
       "hjmbq2n  2wtn1y1m  Auston Matthews scoring a powerplay goal on a ...    45   \n",
       "hjmd23t  41bt33yx  Yo. Jack Campbell keeps the record. Canes lost...    43   \n",
       "hjmi3w6  5i88ndky  Beat the bruins and as soon as it cuts to the ...    40   \n",
       "hjmi4ch  5aceky7e                      Hell week results: Leafs 3-0.    36   \n",
       "hjliwp1     l8jzj   First game I’m going to in like 8 years. LFG GLG    34   \n",
       "\n",
       "                 time                                              words  \\\n",
       "hjmbq2n  1636246625.0  [Auston, Matthews, scoring, a, powerplay, goal...   \n",
       "hjmd23t  1636247261.0  [Yo., Jack, Campbell, keeps, the, record., Can...   \n",
       "hjmi3w6  1636249695.0  [Beat, the, bruins, and, as, soon, as, it, cut...   \n",
       "hjmi4ch  1636249701.0                [Hell, week, results:, Leafs, 3-0.]   \n",
       "hjliwp1  1636233429.0  [First, game, I’m, going, to, in, like, 8, yea...   \n",
       "\n",
       "                                                   cleaned  \n",
       "hjmbq2n  [auston, matthew, score, powerplay, goal, marc...  \n",
       "hjmd23t                  [jack, campbel, keep, cane, lost]  \n",
       "hjmi3w6   [beat, bruin, soon, cut, hab, game, score, good]  \n",
       "hjmi4ch                                 [hell, week, leaf]  \n",
       "hjliwp1                        [first, game, go, lfg, glg]  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data cleanup\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\") + [\"like\", \"get\"]\n",
    "snowball = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "#words = [j.lower() for i in [w.split() for w in comments] for j in i]\n",
    "df['words'] = [token.split() for token in df['body']]\n",
    "\n",
    "g = []\n",
    "for comment in df['words']:\n",
    "    l = []\n",
    "    for token in comment:\n",
    "        if token.isalpha() and token not in stopwords:\n",
    "            l.append(snowball.stem(token.lower()))\n",
    "    g.append(l)\n",
    "\n",
    "df['cleaned'] = g\n",
    "\n",
    "words = [j for i in [w for w in df['cleaned'].tolist()] for j in i]\n",
    "\n",
    "df.head()\n",
    "\n",
    "# various algorithms to get the base of each\n",
    "#snowball_words = [snowball.stem(w) for w in df['cleaned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "61f77502",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T11:46:07.671690Z",
     "start_time": "2021-11-10T11:46:07.569689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total: 2139 negative: 1494 positive: 645\n"
     ]
    }
   ],
   "source": [
    "#Analysis Time!\n",
    "\n",
    "#number of positive vs negative comments\n",
    "pos = 0\n",
    "neg = 0\n",
    "\n",
    "for c in df['cleaned']:\n",
    "    if is_positive(\" \".join(c)): pos += 1 \n",
    "    else: neg += 1\n",
    "\n",
    "print(\"\\ntotal: %d negative: %d positive: %d\"%(len(df['cleaned']), neg, pos)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8472e647",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T11:38:41.189646Z",
     "start_time": "2021-11-10T11:38:41.178200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   i fuck play game look good   go goal  the leaf \n",
      " 294  227  110  109   92   85   85   84   82   75 \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Words used most frequently\n",
    "print(nltk.FreqDist(words).tabulate(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6b1dff8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T11:39:10.486704Z",
     "start_time": "2021-11-10T11:39:10.372015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ('i', 'think')    ('i', 'love') ('holi', 'fuck')  ('love', 'see')    ('i', 'know') \n",
      "              28               25               15               14               13 \n",
      " ('grade', 'a', 'chanc') ('fuck', 'good', 'fuck') ('good', 'fuck', 'good')  ('call', 'ice', 'goal') \n",
      "                       4                        3                        3                        3 \n",
      "   ('fuck', 'good', 'fuck', 'good') ('mcavoy', 'wear', 'littl', 'halo')   ('wear', 'littl', 'halo', 'head') \n",
      "                                  3                                   2                                   2 \n"
     ]
    }
   ],
   "source": [
    "#Word combinations\n",
    "item = words\n",
    "\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(item)\n",
    "bigram_finder.ngram_fd.tabulate(5)\n",
    "\n",
    "trigram_finder = nltk.collocations.TrigramCollocationFinder.from_words(item)\n",
    "trigram_finder.ngram_fd.tabulate(4)\n",
    "\n",
    "quadgram_finder = nltk.collocations.QuadgramCollocationFinder.from_words(item)\n",
    "quadgram_finder.ngram_fd.tabulate(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "31c8369b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T11:45:09.977828Z",
     "start_time": "2021-11-10T11:45:09.908122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refs : 50 total occurrences, 20 positive 30 negative\n",
      "marner : 38 total occurrences, 22 positive 16 negative\n",
      "matthews : 49 total occurrences, 27 positive 22 negative\n",
      "tavares : 13 total occurrences, 4 positive 9 negative\n",
      "nylander : 4 total occurrences, 3 positive 1 negative\n"
     ]
    }
   ],
   "source": [
    "#Words in context. Should be interesting to see the result of this for a variety of words\n",
    "words_of_interest = ['refs', 'marner', 'matthews', 'tavares', 'nylander']\n",
    "\n",
    "for word in words_of_interest:\n",
    "    concordance = nltk.Text(words).concordance_list(snowball.stem(word), lines=None)\n",
    "    print(word, \":\", len(concordance), \"total occurrences,\", end=\" \")\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for entry in concordance:\n",
    "        if is_positive(entry.line): pos += 1\n",
    "        else: neg += 1\n",
    "\n",
    "    print(pos, \"positive\", neg, \"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b705a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba274a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
